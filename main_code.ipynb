{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the required packages ##\n",
    "from sklearn.metrics import recall_score, roc_auc_score, precision_score, f1_score\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training and test sets\n",
    "X_train = np.ndarray\n",
    "X_test = np.ndarray\n",
    "y_train = np.ndarray\n",
    "y_test = np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function you want to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    param = {'n_neighbors': trial.suggest_int('n_neighbors', 1, 10)}\n",
    "\n",
    "    # Use the suggested hyperparameters to initialize the classifier\n",
    "    clf = KNeighborsClassifier(**param)\n",
    "    # Train the classifier based on the given hyperparameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the classifier using the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Specify the evaluation metric you want to optimize the model based on it\n",
    "    metric = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Print the outcomes of each iteration\n",
    "    print(f\"Model Accuracy: {round(metric, 6)}\")\n",
    "    print(f\"Model Parameters: {param}\")\n",
    "    return metric\n",
    "\n",
    "# Use the Optuna to maximize the objective function based on the specified evaluation metrics\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of finished trials\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# Get the outcome of the best trial\n",
    "print(\"Best trial:\")\n",
    "# Define a variable that represents the best trial\n",
    "trial = study.best_trial\n",
    "# Print the best result\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "# Print the best hyperparameters\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function you want to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    param = {'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "             'min_samples_leaf':trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "             'criterion': trial.suggest_categorical('criterion', ['log_loss', 'gini', 'entropy']),\n",
    "             'splitter': trial.suggest_categorical('splitter', ['random', 'best'])}\n",
    "\n",
    "    # Use the suggested hyperparameters to initialize the classifier\n",
    "    clf = DecisionTreeClassifier(**param)\n",
    "    # Train the classifier based on the given hyperparameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the classifier using the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Specify the evaluation metric you want to optimize the model based on it\n",
    "    metric = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Print the outcomes of each iteration\n",
    "    print(f\"Model Accuracy: {round(metric, 6)}\")\n",
    "    print(f\"Model Parameters: {param}\")\n",
    "    return metric\n",
    "\n",
    "# Use the Optuna to maximize the objective function based on the specified evaluation metrics\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of finished trials\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# Get the outcome of the best trial\n",
    "print(\"Best trial:\")\n",
    "# Define a variable that represents the best trial\n",
    "trial = study.best_trial\n",
    "# Print the best result\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "# Print the best hyperparameters\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function you want to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    param = {'n_estimators':trial.suggest_int('n_estimator', 50, 300),\n",
    "             'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "             'min_samples_leaf':trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "             'criterion': trial.suggest_categorical('criterion', ['log_loss', 'gini', 'entropy'])}\n",
    "\n",
    "    # Use the suggested hyperparameters to initialize the classifier\n",
    "    clf = ExtraTreesClassifier(**param)\n",
    "\n",
    "    # Train the classifier based on the given hyperparameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the classifier using the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Specify the evaluation metric you want to optimize the model based on it\n",
    "    metric = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Print the outcomes of each iteration\n",
    "    print(f\"Model Accuracy: {round(metric, 6)}\")\n",
    "    print(f\"Model Parameters: {param}\")\n",
    "    return metric\n",
    "\n",
    "# Use the Optuna to maximize the objective function based on the specified evaluation metrics\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of finished trials\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# Get the outcome of the best trial\n",
    "print(\"Best trial:\")\n",
    "# Define a variable that represents the best trial\n",
    "trial = study.best_trial\n",
    "# Print the best result\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "# Print the best hyperparameters\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function you want to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    param = {'max_depth': trial.suggest_int('max_depth', 1, 20),\n",
    "             'min_samples_leaf':trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "             'criterion': trial.suggest_categorical('criterion', ['log_loss', 'gini', 'entropy']),\n",
    "             'n_estimators': trial.suggest_int('n_estimators', 100, 1000)}\n",
    "\n",
    "    # Use the suggested hyperparameters to initialize the classifier\n",
    "    clf = RandomForestClassifier(**param)\n",
    "\n",
    "    # Train the classifier based on the given hyperparameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the classifier using the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Specify the evaluation metric you want to optimize the model based on it\n",
    "    metric = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Print the outcomes of each iteration\n",
    "    print(f\"Model Accuracy: {round(metric, 6)}\")\n",
    "    print(f\"Model Parameters: {param}\")\n",
    "    return metric\n",
    "\n",
    "# Use the Optuna to maximize the objective function based on the specified evaluation metrics\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of finished trials\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# Get the outcome of the best trial\n",
    "print(\"Best trial:\")\n",
    "# Define a variable that represents the best trial\n",
    "trial = study.best_trial\n",
    "# Print the best result\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "# Print the best hyperparameters\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function you want to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    param = {'n_estimators': trial.suggest_int('n_estimators', 25, 500),\n",
    "             'learning_rate':trial.suggest_float('learning_rate', 0.1, 10)}\n",
    "\n",
    "    # Use the suggested hyperparameters to initialize the classifier\n",
    "    clf = AdaBoostClassifier(**param)\n",
    "\n",
    "    # Train the classifier based on the given hyperparameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the classifier using the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Specify the evaluation metric you want to optimize the model based on it\n",
    "    metric = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Print the outcomes of each iteration\n",
    "    print(f\"Model Accuracy: {round(metric, 6)}\")\n",
    "    print(f\"Model Parameters: {param}\")\n",
    "    return metric\n",
    "\n",
    "# Use the Optuna to maximize the objective function based on the specified evaluation metrics\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of finished trials\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# Get the outcome of the best trial\n",
    "print(\"Best trial:\")\n",
    "# Define a variable that represents the best trial\n",
    "trial = study.best_trial\n",
    "# Print the best result\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "# Print the best hyperparameters\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function you want to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    param = {'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "             'n_estimators':trial.suggest_int('n_estimators', 50, 1000),\n",
    "             'criterion': trial.suggest_categorical('criterion', ['friedman_mse', 'squared_error']),\n",
    "             'learning_rate': trial.suggest_float('learning_rate', 0.1, 5)}\n",
    "\n",
    "    # Use the suggested hyperparameters to initialize the classifier\n",
    "    clf = GradientBoostingClassifier(**param)\n",
    "\n",
    "    # Train the classifier based on the given hyperparameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the classifier using the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    metric = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Print the outcomes of each iteration\n",
    "    print(f\"Model Accuracy: {round(metric, 6)}\")\n",
    "    print(f\"Model Parameters: {param}\")\n",
    "    return metric\n",
    "\n",
    "# Use the Optuna to maximize the objective function based on the specified evaluation metrics\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of finished trials\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# Get the outcome of the best trial\n",
    "print(\"Best trial:\")\n",
    "# Define a variable that represents the best trial\n",
    "trial = study.best_trial\n",
    "# Print the best result\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "# Print the best hyperparameters\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function you want to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    param = {'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "             'gamma':trial.suggest_float('gamma', 0.1, 10),\n",
    "             'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n",
    "             'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 5),\n",
    "             'subsample': trial.suggest_float('subsample', 0.1, 0.9),\n",
    "             'learning_rate': trial.suggest_float('learning_rate', 0.1, 5)}\n",
    "\n",
    "    # Use the suggested hyperparameters to initialize the classifier\n",
    "    clf = XGBClassifier(**param)\n",
    "    \n",
    "    # Train the classifier based on the given hyperparameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the classifier using the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Specify the evaluation metric you want to optimize the model based on it\n",
    "    metric = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # Print the outcomes of each iteration\n",
    "    print(f\"Model Accuracy: {round(metric, 6)}\")\n",
    "    print(f\"Model Parameters: {param}\")\n",
    "    return metric\n",
    "\n",
    "# Use the Optuna to maximize the objective function based on the specified evaluation metrics\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of finished trials\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# Get the outcome of the best trial\n",
    "print(\"Best trial:\")\n",
    "# Define a variable that represents the best trial\n",
    "trial = study.best_trial\n",
    "# Print the best result\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "# Print the best hyperparameters\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function you want to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    param = {'num_leaves': trial.suggest_int('num_leaves', 2, 50),\n",
    "             'max_depth': trial.suggest_int('max_depth', 1, 30), # -1\n",
    "             'learning_rate': trial.suggest_float('learning_rate', 0.01, 5),\n",
    "             'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "             'subsample_for_bin': trial.suggest_int('subsample_for_bin', 50,180),\n",
    "             'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.1, 8)}\n",
    "\n",
    "    # Use the suggested hyperparameters to initialize the classifier\n",
    "    clf = LGBMClassifier(**param, random_state=1, objective='binary')\n",
    "    \n",
    "    # Train the classifier based on the given hyperparameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the classifier using the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Specify the evaluation metric you want to optimize the model based on it\n",
    "    metric = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Print the outcomes of each iteration\n",
    "    print(f\"Model Accuracy: {round(metric, 6)}\")\n",
    "    print(f\"Model Parameters: {param}\")\n",
    "    return metric\n",
    "\n",
    "# Use the Optuna to maximize the objective function based on the specified evaluation metrics\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of finished trials\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# Get the outcome of the best trial\n",
    "print(\"Best trial:\")\n",
    "# Define a variable that represents the best trial\n",
    "trial = study.best_trial\n",
    "# Print the best result\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "# Print the best hyperparameters\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function you want to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the range of the hyperparameters you want to use in the optimization process\n",
    "    param = {'iterations': trial.suggest_int('iterations', 2, 100),\n",
    "             'depth': trial.suggest_int('depth', 1, 16), # -1\n",
    "             'learning_rate': trial.suggest_float('learning_rate', 0.001, 5),\n",
    "             'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 9)}\n",
    "    \n",
    "    # Use the suggested hyperparameters to initialize the classifier\n",
    "    clf = CatBoostClassifier(**param, random_state=1, custom_metric=['F1', 'AUC'])\n",
    "    \n",
    "    # Train the classifier based on the given hyperparameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the classifier using the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Specify the evaluation metric you want to optimize the model based on it\n",
    "    metric = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Print the outcomes of each iteration\n",
    "    print(f\"Model Accuracy: {round(metric, 6)}\")\n",
    "    print(f\"Model Parameters: {param}\")\n",
    "    return metric\n",
    "\n",
    "# Use the Optuna to maximize the objective function based on the specified evaluation metrics\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of finished trials\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# Get the outcome of the best trial\n",
    "print(\"Best trial:\")\n",
    "# Define a variable that represents the best trial\n",
    "trial = study.best_trial\n",
    "# Print the best result\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "# Print the best hyperparameters\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
